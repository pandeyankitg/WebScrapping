{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiYUDI9Z5r9T"
      },
      "outputs": [],
      "source": [
        "'''Sraping ESPN College Football data \n",
        "http://www.espn.com/college-sports/football/recruiting/databaseresults/_/sportid/24/class/2006/sort/school/starsfilter/GT/ratingfilter/GT/statuscommit/Commitments/statusuncommit/Uncommited\n",
        "The script will loop through a defined number of pages to extract footballer data. \n",
        "'''\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os \n",
        "import os.path\n",
        "import csv \n",
        "import time \n",
        "\n",
        "\n",
        "def writerows(rows, filename):\n",
        "    with open(filename, 'a', encoding='utf-8') as toWrite:\n",
        "        writer = csv.writer(toWrite)\n",
        "        writer.writerows(rows)\n",
        " \n",
        "\n",
        "def getlistings(listingurl):\n",
        "    '''\n",
        "    scrap footballer data from the page and write to CSV\n",
        "    '''\n",
        "\n",
        "    # prepare headers\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'}\n",
        "\n",
        "    # fetching the url, raising error if operation fails\n",
        "    try:\n",
        "        response = requests.get(listingurl, headers=headers)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(e)\n",
        "        exit()\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    listings = []\n",
        "\n",
        "    # loop through the table, get data from the columns\n",
        "    for rows in soup.find_all(\"tr\"):\n",
        "        if (\"oddrow\" in rows[\"class\"]) or (\"evenrow\" in rows[\"class\"]):          \n",
        "                        \n",
        "            name = rows.find(\"div\", class_=\"name\").a.get_text()\n",
        "            hometown = rows.find_all(\"td\")[1].get_text()\n",
        "            school = hometown[hometown.find(\",\")+4:]\n",
        "            city = hometown[:hometown.find(\",\")+4]\n",
        "            position = rows.find_all(\"td\")[2].get_text()\n",
        "            grade = rows.find_all(\"td\")[4].get_text()\n",
        "\n",
        "            # append data to the list\n",
        "            listings.append([name, school, city, position, grade])\n",
        "\n",
        "    return listings\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    '''\n",
        "    Set CSV file name. \n",
        "    Remove if file alreay exists to ensure a fresh start\n",
        "    '''\n",
        "    filename = \"footballers.csv\"\n",
        "    if os.path.exists(filename):\n",
        "        os.remove(filename)\n",
        "    \n",
        "    '''\n",
        "    Url to fetch consists of 3 parts:\n",
        "    baseurl, page number, year, remaining url\n",
        "    '''\n",
        "    baseurl = \"http://www.espn.com/college-sports/football/recruiting/databaseresults/_/page/\" \n",
        "    page = 1\n",
        "    parturl = \"/sportid/24/class/2006/sort/school/starsfilter/GT/ratingfilter/GT/statuscommit/Commitments/statusuncommit/Uncommited\"\n",
        "\n",
        "    # scrap all pages\n",
        "    while page < 259:\n",
        "        listingurl = baseurl + str(page) + parturl\n",
        "        listings = getlistings(listingurl)\n",
        "\n",
        "        # write to CSV        \n",
        "        writerows(listings, filename)\n",
        "\n",
        "        # take a break\n",
        "        time.sleep(3)\n",
        "\n",
        "        page += 1\n",
        "\n",
        "if page > 1:\n",
        "    print(\"Listings fetched successfully.\")"
      ]
    }
  ]
}